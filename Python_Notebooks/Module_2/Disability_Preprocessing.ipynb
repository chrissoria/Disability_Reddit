{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bab0b9a6-f0a2-43b6-9aa2-7b81ea978de8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/chrissoria/Documents/Programming /Data-Science-Social-Justice-main/data'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.chdir('../../data')\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f770f74-2cc0-4c90-a765-0226804b76e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.read_csv('disability_30000_submissions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "726c9ac6-3a86-4294-90bf-fb1f9c2ebc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df\t os\t pd\t \n"
     ]
    }
   ],
   "source": [
    "%who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "befb4dc1-3b27-4bbb-b5d7-89ac241fdca1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>author</th>\n",
       "      <th>over_18</th>\n",
       "      <th>title</th>\n",
       "      <th>is_self</th>\n",
       "      <th>selftext</th>\n",
       "      <th>score</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>num_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>p8hcfj</td>\n",
       "      <td>1629505393</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OrdanCoal</td>\n",
       "      <td>False</td>\n",
       "      <td>Anybody noticed the trap people set with your ...</td>\n",
       "      <td>True</td>\n",
       "      <td>Like, if I'm crying because I'm in chronic pai...</td>\n",
       "      <td>34</td>\n",
       "      <td>0.92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>p84kgg</td>\n",
       "      <td>1629463903</td>\n",
       "      <td>Rant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dusnood_</td>\n",
       "      <td>False</td>\n",
       "      <td>Feeling guilty for spending with \"disability i...</td>\n",
       "      <td>True</td>\n",
       "      <td>I don’t have any regular bills as I still live...</td>\n",
       "      <td>32</td>\n",
       "      <td>0.94</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>p8z4a1</td>\n",
       "      <td>1629579321</td>\n",
       "      <td>Image</td>\n",
       "      <td>NaN</td>\n",
       "      <td>sushimewmew</td>\n",
       "      <td>False</td>\n",
       "      <td>My old braces vs. my new brace. I'm very happy...</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>17</td>\n",
       "      <td>0.96</td>\n",
       "      <td>NaN</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>p82mfs</td>\n",
       "      <td>1629455803</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ShenandoahValley</td>\n",
       "      <td>False</td>\n",
       "      <td>Only 30% of SSD applicants approved on first try</td>\n",
       "      <td>True</td>\n",
       "      <td>This came from a manager at the local SSA offi...</td>\n",
       "      <td>14</td>\n",
       "      <td>0.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>p8cdd2</td>\n",
       "      <td>1629488445</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>musicalearnightingal</td>\n",
       "      <td>False</td>\n",
       "      <td>Have you ever called someone out for treating ...</td>\n",
       "      <td>True</td>\n",
       "      <td>So one of my coworkers really hates me. I get ...</td>\n",
       "      <td>12</td>\n",
       "      <td>0.93</td>\n",
       "      <td>NaN</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0      id  created_utc link_flair_text author_flair_text  \\\n",
       "0           0  p8hcfj   1629505393             NaN               NaN   \n",
       "1           1  p84kgg   1629463903            Rant               NaN   \n",
       "2           2  p8z4a1   1629579321           Image               NaN   \n",
       "3           3  p82mfs   1629455803      Discussion               NaN   \n",
       "4           4  p8cdd2   1629488445             NaN               NaN   \n",
       "\n",
       "                 author  over_18  \\\n",
       "0             OrdanCoal    False   \n",
       "1              dusnood_    False   \n",
       "2           sushimewmew    False   \n",
       "3      ShenandoahValley    False   \n",
       "4  musicalearnightingal    False   \n",
       "\n",
       "                                               title  is_self  \\\n",
       "0  Anybody noticed the trap people set with your ...     True   \n",
       "1  Feeling guilty for spending with \"disability i...     True   \n",
       "2  My old braces vs. my new brace. I'm very happy...    False   \n",
       "3   Only 30% of SSD applicants approved on first try     True   \n",
       "4  Have you ever called someone out for treating ...     True   \n",
       "\n",
       "                                            selftext  score  upvote_ratio  \\\n",
       "0  Like, if I'm crying because I'm in chronic pai...     34          0.92   \n",
       "1  I don’t have any regular bills as I still live...     32          0.94   \n",
       "2                                                NaN     17          0.96   \n",
       "3  This came from a manager at the local SSA offi...     14          0.90   \n",
       "4  So one of my coworkers really hates me. I get ...     12          0.93   \n",
       "\n",
       "  distinguished  num_comments  \n",
       "0           NaN            53  \n",
       "1           NaN            65  \n",
       "2           NaN            43  \n",
       "3           NaN            81  \n",
       "4           NaN            20  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe0ae0db-1fc0-4a0d-80d0-2c0fc1acc095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30339, 14)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ed1e074a-2eb5-4650-a54d-80ca158d674a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Unnamed: 0',\n",
       " 'id',\n",
       " 'created_utc',\n",
       " 'link_flair_text',\n",
       " 'author_flair_text',\n",
       " 'author',\n",
       " 'over_18',\n",
       " 'title',\n",
       " 'is_self',\n",
       " 'selftext',\n",
       " 'score',\n",
       " 'upvote_ratio',\n",
       " 'distinguished',\n",
       " 'num_comments']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9e4e1445-fb61-4b59-8f0e-cdc73711fca3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RangeIndex(start=0, stop=14928, step=1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df.loc[~df['selftext'].isin(['[removed]', '[deleted]' ]),:]\n",
    "df = df.dropna(subset=['selftext']).reset_index()\n",
    "df.index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c4cc75-faf4-4507-af6e-0d015d53b4e5",
   "metadata": {},
   "source": [
    "The dataset goes from 30,339 to 14,928 after deleting all removed and deleted selftext inputs and Null values. This seems like a dramatic reduction. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f944651-1941-43ce-84dc-b36f8a4e0bca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Like, if I'm crying because I'm in chronic pain, it's because I'm \"too weak to be happy\"\n",
      "\n",
      "If I'm happy and smile, it means I couldn't possibly be in pain\n",
      "\n",
      "The fucked up thing is, I can definitely be in pain and still be happy, but if I have to meet one more nasty person that I thought I would like, but turns out they're horrible Ableist, I might vomit. \n",
      "\n",
      "The people that know me know me as the person that is legitimately in pain **and** is still smiling, but then these people that I would have vouched for as neighbors, friends, and otherwise the kind of people you'd genuinely want to meet, suddenly turned out to be complete bigots. \n",
      "\n",
      "It's like coming to find out a dear member of your family is, like, **dangerously** racist, or that your Uncle doesn't see women as people. \n",
      "\n",
      "I think one of the worst things being disabled and, for some of us, debatably, perhaps *the* worst thing, is the faith you lose not just in life, but in humanity. \n",
      "\n",
      "Like, I was okay with being on a tiny spinning rock on the edge of a centerless void, but then to realize the little company we do have to keep, our fellows, don't really care, either? \n",
      "\n",
      "I mean, that's...\n",
      "\n",
      "worse...\n",
      "\n",
      "that's definitely worse\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# Load the English preprocessing pipeline\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Parse the first reddit post in the dataset\n",
    "parsed_post = nlp(df.selftext[0])\n",
    "print(parsed_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4a640c84-6009-413c-969b-aeb386885ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1\n",
      "Like, if I'm crying because I'm in chronic pain, it's because I'm \"too weak to be happy\"\n",
      "\n",
      "If I'm happy and smile, it means I couldn't possibly be in pain\n",
      "\n",
      "\n",
      "\n",
      "Sentence 2\n",
      "The fucked up thing is, I can definitely be in pain and still be happy, but if I have to meet one more nasty person that I thought I would like, but turns out they're horrible Ableist, I might vomit.\n",
      "\n",
      "Sentence 3\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sentence 4\n",
      "The people that know me know me as the person that is legitimately in pain **and** is still smiling, but then these people that I would have vouched for as neighbors, friends, and otherwise the kind of people you'd genuinely want to meet, suddenly turned out to be complete bigots.\n",
      "\n",
      "Sentence 5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sentence 6\n",
      "It's like coming to find out a dear member of your family is, like, **dangerously** racist, or that your Uncle doesn't see women as people.\n",
      "\n",
      "Sentence 7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sentence 8\n",
      "I think one of the worst things being disabled and, for some of us, debatably, perhaps *the* worst thing, is the faith you lose not just in life, but in humanity.\n",
      "\n",
      "Sentence 9\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Sentence 10\n",
      "Like, I was okay with being on a tiny spinning rock on the edge of a centerless void, but then to realize the little company we do have to keep, our fellows, don't really care, either? \n",
      "\n",
      "\n",
      "\n",
      "Sentence 11\n",
      "I mean, that's...\n",
      "\n",
      "worse...\n",
      "\n",
      "that's definitely worse\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, sentence in enumerate(parsed_post.sents):\n",
    "    print(f'Sentence {idx + 1}')\n",
    "    print(sentence)\n",
    "    print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e662c63c-1ee2-485a-804d-a90836e5d3f9",
   "metadata": {},
   "source": [
    "This actually did a decent job of caputring \"sentences.\" The two exceptions are Sentence 7 and Sentence 9, both blank. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a8c7a16-3eeb-41e9-a123-d2d78f4f6ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token_text</th>\n",
       "      <th>part_of_speech</th>\n",
       "      <th>token_lemma</th>\n",
       "      <th>token_stop</th>\n",
       "      <th>token_punct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Like</td>\n",
       "      <td>INTJ</td>\n",
       "      <td>like</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>,</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>if</td>\n",
       "      <td>SCONJ</td>\n",
       "      <td>if</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I</td>\n",
       "      <td>PRON</td>\n",
       "      <td>I</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'m</td>\n",
       "      <td>AUX</td>\n",
       "      <td>be</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>crying</td>\n",
       "      <td>VERB</td>\n",
       "      <td>cry</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>because</td>\n",
       "      <td>SCONJ</td>\n",
       "      <td>because</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I</td>\n",
       "      <td>PRON</td>\n",
       "      <td>I</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>'m</td>\n",
       "      <td>AUX</td>\n",
       "      <td>be</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>in</td>\n",
       "      <td>ADP</td>\n",
       "      <td>in</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>chronic</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>chronic</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>pain</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>pain</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>,</td>\n",
       "      <td>PUNCT</td>\n",
       "      <td>,</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>it</td>\n",
       "      <td>PRON</td>\n",
       "      <td>it</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>'s</td>\n",
       "      <td>AUX</td>\n",
       "      <td>be</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   token_text part_of_speech token_lemma  token_stop  token_punct\n",
       "0        Like           INTJ        like       False        False\n",
       "1           ,          PUNCT           ,       False         True\n",
       "2          if          SCONJ          if        True        False\n",
       "3           I           PRON           I        True        False\n",
       "4          'm            AUX          be        True        False\n",
       "5      crying           VERB         cry       False        False\n",
       "6     because          SCONJ     because        True        False\n",
       "7           I           PRON           I        True        False\n",
       "8          'm            AUX          be        True        False\n",
       "9          in            ADP          in        True        False\n",
       "10    chronic            ADJ     chronic       False        False\n",
       "11       pain           NOUN        pain       False        False\n",
       "12          ,          PUNCT           ,       False         True\n",
       "13         it           PRON          it        True        False\n",
       "14         's            AUX          be        True        False"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract the first 15 items for the following properties of the parsed post\n",
    "\n",
    "# The token text \n",
    "token_text = [token.orth_ for token in parsed_post][:15]   \n",
    "# Part of speech \n",
    "token_pos = [token.pos_ for token in parsed_post][:15]   \n",
    "# Lemma (or 'dictionary form')\n",
    "token_lemma = [token.lemma_ for token in parsed_post][:15]\n",
    "# Stop word? t/f\n",
    "token_stop = [token.is_stop for token in parsed_post][:15]\n",
    "# Puncutation? t/f\n",
    "token_punct = [token.is_punct for token in parsed_post][:15]\n",
    "\n",
    "# Make a dataframe with these items\n",
    "pd.DataFrame(zip(token_text, token_pos, token_lemma, token_stop, token_punct),\n",
    "             columns=['token_text', 'part_of_speech', 'token_lemma', 'token_stop', 'token_punct'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d972af6-a605-4ebd-bbe1-e30db9db13aa",
   "metadata": {},
   "source": [
    "It seems to be correctly identifying things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "70271e31-0f01-4ea4-8b59-ffef04405fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(token):\n",
    "    \"\"\"Helper function that specifies whether a token is:\n",
    "        - punctuation\n",
    "        - space\n",
    "        - digit\n",
    "    \"\"\"\n",
    "    return token.is_punct or token.is_space or token.is_digit\n",
    "\n",
    "def line_read(df, text_col='selftext'):\n",
    "    \"\"\"Generator function to read in text from df and get rid of line breaks.\"\"\"    \n",
    "    for text in df[text_col]:\n",
    "        yield text.replace('\\n', '')\n",
    "\n",
    "def preprocess(df, text_col='selftext', allowed_postags=['NOUN', 'ADJ']):\n",
    "    \"\"\"Preprocessing function to apply to a dataframe.\"\"\"\n",
    "    for parsed in nlp.pipe(line_read(df, text_col), batch_size=1000, disable=[\"tok2vec\", \"ner\"]):\n",
    "        # Gather lowercased, lemmatized tokens\n",
    "        tokens = [token.lemma_.lower() if token.lemma_ != '-PRON-'\n",
    "                  else token.lower_ \n",
    "                  for token in parsed if not clean(token)]\n",
    "        # Remove specific lemmatizations, and words that are not nouns or adjectives\n",
    "        tokens = [lemma\n",
    "                  for lemma in tokens\n",
    "                  if not lemma in [\"'s\",  \"’s\", \"’\"] and not lemma in allowed_postags]\n",
    "        # Remove stop words\n",
    "        tokens = [token for token in tokens if token not in spacy.lang.en.stop_words.STOP_WORDS]\n",
    "        yield tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2326ad09-8e34-45c5-a4a5-198a72504964",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmas = [line for line in preprocess(df)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dfa5da7c-76a4-448d-8121-637caa136b52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clean\t df\t idx\t lemmas\t line_read\t nlp\t os\t parsed_post\t pd\t \n",
      "preprocess\t sentence\t spacy\t token_lemma\t token_pos\t token_punct\t token_stop\t token_text\t \n"
     ]
    }
   ],
   "source": [
    "%who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a219c079-35db-4ceb-86bf-d29146b41591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['like',\n",
       " 'crying',\n",
       " 'chronic',\n",
       " 'pain',\n",
       " 'weak',\n",
       " 'happy\"if',\n",
       " 'happy',\n",
       " 'smile',\n",
       " 'means',\n",
       " 'possibly',\n",
       " 'painthe',\n",
       " 'fucked',\n",
       " 'thing',\n",
       " 'definitely',\n",
       " 'pain',\n",
       " 'happy',\n",
       " 'meet',\n",
       " 'nasty',\n",
       " 'person',\n",
       " 'thought',\n",
       " 'like',\n",
       " 'turns',\n",
       " 'horrible',\n",
       " 'ableist',\n",
       " 'vomit',\n",
       " 'people',\n",
       " 'know',\n",
       " 'know',\n",
       " 'person',\n",
       " 'legitimately',\n",
       " 'pain',\n",
       " 'smiling',\n",
       " 'people',\n",
       " 'vouched',\n",
       " 'neighbors',\n",
       " 'friends',\n",
       " 'kind',\n",
       " 'people',\n",
       " 'genuinely',\n",
       " 'want',\n",
       " 'meet',\n",
       " 'suddenly',\n",
       " 'turned',\n",
       " 'complete',\n",
       " 'bigots',\n",
       " 'like',\n",
       " 'coming',\n",
       " 'find',\n",
       " 'dear',\n",
       " 'member',\n",
       " 'family',\n",
       " 'like',\n",
       " 'dangerously',\n",
       " 'racist',\n",
       " 'uncle',\n",
       " 'women',\n",
       " 'people',\n",
       " 'think',\n",
       " 'worst',\n",
       " 'things',\n",
       " 'disabled',\n",
       " 'debatably',\n",
       " 'worst',\n",
       " 'thing',\n",
       " 'faith',\n",
       " 'lose',\n",
       " 'life',\n",
       " 'humanity',\n",
       " 'like',\n",
       " 'okay',\n",
       " 'tiny',\n",
       " 'spinning',\n",
       " 'rock',\n",
       " 'edge',\n",
       " 'centerless',\n",
       " 'void',\n",
       " 'realize',\n",
       " 'little',\n",
       " 'company',\n",
       " 'fellows',\n",
       " 'care',\n",
       " 'mean',\n",
       " \"that's\",\n",
       " 'worse',\n",
       " 'definitely',\n",
       " 'worse']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f1e50d8-e3d7-45af-b913-82402b852f48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title, basically? An attorney advised me to find services/products that help me perform my part-time job, and that I’d be able to deduct their cost from my earnings for the process of determining eligibility.\n",
      "\n",
      "I’ll examine my own life to see what I might be able to cite, but I’m at a loss and would love to hear examples of what other people have cited!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['title',\n",
       " 'basically',\n",
       " 'attorney',\n",
       " 'advised',\n",
       " 'find',\n",
       " 'services',\n",
       " 'products',\n",
       " 'help',\n",
       " 'perform',\n",
       " 'time',\n",
       " 'job',\n",
       " 'able',\n",
       " 'deduct',\n",
       " 'cost',\n",
       " 'earnings',\n",
       " 'process',\n",
       " 'determining',\n",
       " 'eligibility',\n",
       " 'i’ll',\n",
       " 'examine',\n",
       " 'life',\n",
       " 'able',\n",
       " 'cite',\n",
       " 'loss',\n",
       " 'love',\n",
       " 'hear',\n",
       " 'examples',\n",
       " 'people',\n",
       " 'cited']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parsed_post_test = nlp(df.selftext[10])\n",
    "print(parsed_post_test)\n",
    "lemmas[10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a129129f-6e50-4add-8fcd-e53f6fa142f6",
   "metadata": {},
   "source": [
    "The lemmas here look different than the post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cb132fcf-8c05-4241-a875-02f8fc51dfa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title, basically? An attorney advised me to find services/products that help me perform my part-time job, and that I’d be able to deduct their cost from my earnings for the process of determining eligibility.\n",
      "\n",
      "I’ll examine my own life to see what I might be able to cite, but I’m at a loss and would love to hear examples of what other people have cited!\n"
     ]
    }
   ],
   "source": [
    "print(df.selftext[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "61993f69-f244-4eb7-a2c1-867d9d80823f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=14928, step=1)\n"
     ]
    }
   ],
   "source": [
    "print(df.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "147ba991-0654-4d85-ae88-b2eef466f362",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "bigram = Phrases(lemmas, min_count=10, threshold=100)\n",
    "trigram = Phrases(bigram[lemmas], min_count=10, threshold=50)  \n",
    "bigram_phraser = Phraser(bigram)\n",
    "trigram_phraser = Phraser(trigram)\n",
    "\n",
    "# Form trigrams\n",
    "trigrams = [trigram_phraser[bigram_phraser[doc]] for doc in lemmas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d1d5b04-6313-4d9b-b923-255f182ab055",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'like crying chronic_pain weak happy\"if happy smile means possibly painthe fucked thing definitely pain happy meet nasty person thought like turns horrible ableist vomit people know know person legitimately pain smiling people vouched neighbors friends kind people genuinely want meet suddenly turned complete bigots like coming find dear member family like dangerously racist uncle women people think worst things disabled debatably worst thing faith lose life humanity like okay tiny spinning rock edge centerless void realize little company fellows care mean that\\'s worse definitely worse'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigrams_joined = [' '.join(trigram) for trigram in trigrams]\n",
    "trigrams_joined[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "478fe569-0a44-4dc9-97af-513e8d2ac4e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['That', 'was', 'not', 'a', 'big', 'deal']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_phraser[\"That\", \"was\", \"not\", \"a\", \"big\", \"deal\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5218add0-be58-4200-b2d6-1b977f54d97a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "423"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bigram_phraser.phrasegrams.keys()) #423 Bigrams were identified in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8c53ba3a-eacf-4a9f-9642-3bd6b1fd828c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['thanks_advance',\n",
       " 'muscular_dystrophy',\n",
       " 'official_diagnosis',\n",
       " 'onset_date',\n",
       " 'bachelor_degree',\n",
       " 'wheelchair_user',\n",
       " 'muscle_weakness',\n",
       " 'l4_l5',\n",
       " 'spinal_cord',\n",
       " 'handicap_parking']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(bigram_phraser.phrasegrams.keys())[:10] #what is the 10 corresponding to? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1891d197-5131-4bd8-86eb-c2b34c947703",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['substantial_gainful_activity',\n",
       " 'spinal_cord_injury',\n",
       " 'major_depressive_disorder',\n",
       " 'generalized_anxiety_disorder',\n",
       " 'ambulatory_wheelchair_user',\n",
       " 'borderline_personality_disorder',\n",
       " 'fetal_alcohol_syndrome',\n",
       " 'playing_video_games',\n",
       " 'administrative_law_judge',\n",
       " 'obsessive_compulsive_disorder',\n",
       " 'social_security_administration',\n",
       " 'parked_handicap_spot',\n",
       " 'auditory_processing_disorder',\n",
       " 'short_term_memory',\n",
       " 'play_video_games',\n",
       " 'traumatic_brain_injury',\n",
       " 'assisted_living_facility',\n",
       " 'greatly_appreciated_thank',\n",
       " '\\u200d_♀_️',\n",
       " 'mild_cerebral_palsy',\n",
       " 'ehlers_danlos_syndrome',\n",
       " 'walk_short_distances',\n",
       " 'residual_functional_capacity',\n",
       " 'degenerative_disc_disease',\n",
       " 'brother_cerebral_palsy',\n",
       " 'mixed_connective_tissue',\n",
       " 'remotely_fullest_extent',\n",
       " 'spinal_muscular_atrophy',\n",
       " 'california_department_rehabilitation']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[trigram for trigram in list(trigram_phraser.phrasegrams.keys()) if trigram.count('_') == 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5bb64dc8-9135-4a5f-93b8-dfdb325c997c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserting next to selftext column\n",
    "df.insert(loc=7, column='lemmas', value=trigrams_joined)\n",
    "# Removing empty rows in lemmas\n",
    "df = df[~df['lemmas'].isin([''])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f6aa7d23-a9b3-4c45-8bb8-97fe24109a03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>created_utc</th>\n",
       "      <th>link_flair_text</th>\n",
       "      <th>author_flair_text</th>\n",
       "      <th>author</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>over_18</th>\n",
       "      <th>title</th>\n",
       "      <th>is_self</th>\n",
       "      <th>selftext</th>\n",
       "      <th>score</th>\n",
       "      <th>upvote_ratio</th>\n",
       "      <th>distinguished</th>\n",
       "      <th>num_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>p8hcfj</td>\n",
       "      <td>1629505393</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>OrdanCoal</td>\n",
       "      <td>like crying chronic_pain weak happy\"if happy s...</td>\n",
       "      <td>False</td>\n",
       "      <td>Anybody noticed the trap people set with your ...</td>\n",
       "      <td>True</td>\n",
       "      <td>Like, if I'm crying because I'm in chronic pai...</td>\n",
       "      <td>34</td>\n",
       "      <td>0.92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>p84kgg</td>\n",
       "      <td>1629463903</td>\n",
       "      <td>Rant</td>\n",
       "      <td>NaN</td>\n",
       "      <td>dusnood_</td>\n",
       "      <td>regular bills live relatives pay small things ...</td>\n",
       "      <td>False</td>\n",
       "      <td>Feeling guilty for spending with \"disability i...</td>\n",
       "      <td>True</td>\n",
       "      <td>I don’t have any regular bills as I still live...</td>\n",
       "      <td>32</td>\n",
       "      <td>0.94</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>p82mfs</td>\n",
       "      <td>1629455803</td>\n",
       "      <td>Discussion</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ShenandoahValley</td>\n",
       "      <td>came manager local ssa office va spoke started...</td>\n",
       "      <td>False</td>\n",
       "      <td>Only 30% of SSD applicants approved on first try</td>\n",
       "      <td>True</td>\n",
       "      <td>This came from a manager at the local SSA offi...</td>\n",
       "      <td>14</td>\n",
       "      <td>0.90</td>\n",
       "      <td>NaN</td>\n",
       "      <td>81</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  Unnamed: 0      id  created_utc link_flair_text author_flair_text  \\\n",
       "0      0           0  p8hcfj   1629505393             NaN               NaN   \n",
       "1      1           1  p84kgg   1629463903            Rant               NaN   \n",
       "2      3           3  p82mfs   1629455803      Discussion               NaN   \n",
       "\n",
       "             author                                             lemmas  \\\n",
       "0         OrdanCoal  like crying chronic_pain weak happy\"if happy s...   \n",
       "1          dusnood_  regular bills live relatives pay small things ...   \n",
       "2  ShenandoahValley  came manager local ssa office va spoke started...   \n",
       "\n",
       "   over_18                                              title  is_self  \\\n",
       "0    False  Anybody noticed the trap people set with your ...     True   \n",
       "1    False  Feeling guilty for spending with \"disability i...     True   \n",
       "2    False   Only 30% of SSD applicants approved on first try     True   \n",
       "\n",
       "                                            selftext  score  upvote_ratio  \\\n",
       "0  Like, if I'm crying because I'm in chronic pai...     34          0.92   \n",
       "1  I don’t have any regular bills as I still live...     32          0.94   \n",
       "2  This came from a manager at the local SSA offi...     14          0.90   \n",
       "\n",
       "  distinguished  num_comments  \n",
       "0           NaN            53  \n",
       "1           NaN            65  \n",
       "2           NaN            81  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d3e34ef0-2c56-4c07-aa3e-b3ba90f5f616",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save to new csv\n",
    "df.to_csv('disability_sub_top_sm_lemmas.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d90474-cebe-49ce-839b-d1b3623b256c",
   "metadata": {},
   "source": [
    "Why did it generate 30,000 entries instead of 16,000 like we had ended up with after filter nulls and na?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "823a98a2-bfe3-41f8-bf8e-ab9248cbc12f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phraser\t Phrases\t bigram\t bigram_phraser\t clean\t df\t idx\t lemmas\t line_read\t \n",
      "nlp\t os\t parsed_post\t parsed_post_test\t pd\t preprocess\t sentence\t spacy\t token_lemma\t \n",
      "token_pos\t token_punct\t token_stop\t token_text\t trigram\t trigram_phraser\t trigrams\t trigrams_joined\t \n"
     ]
    }
   ],
   "source": [
    "%who"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5bba43-852b-48b6-b1d0-364da232dc1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
